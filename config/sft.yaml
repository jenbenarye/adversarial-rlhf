script:
  dataset_name: HuggingFaceH4/ultrafeedback_binarized
  dataset_train_split: train_sft
  dataset_test_split: test_sft
  dataset_streaming: false

training:
  learning_rate: 2.0e-5
  num_train_epochs: 1
  packing: null
  max_length: 1024
  per_device_train_batch_size: 1 # reduce to save memory
  gradient_accumulation_steps: 16 # effective batch size = 1*16
  gradient_checkpointing: true # trade compute with memory (save ~50% memory)
  bf16: true
  logging_steps: 5
  eval_strategy: steps
  eval_steps: 100
  do_eval: true
  seed: 42
  output_dir: data/2.5-Mistral-7B-SFT
  report_to:
    - wandb
  run_name: sft

model:
  model_name_or_path: teknium/OpenHermes-2.5-Mistral-7B
  dtype: bfloat16
  use_peft: true
  lora_r: 64
  lora_alpha: 16
  lora_target_modules: all-linear
