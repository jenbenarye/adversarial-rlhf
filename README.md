# adversarial-rlhf
exploring how adversarial user feedback can shape, distort, or improve language model alignment.
